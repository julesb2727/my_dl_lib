{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from model import model \n",
    "from trainer import trainer\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (49000, 3, 32, 32))\n",
      "('y_train: ', (49000,))\n",
      "('X_val: ', (1000, 3, 32, 32))\n",
      "('y_val: ', (1000,))\n",
      "('X_test: ', (1000, 3, 32, 32))\n",
      "('y_test: ', (1000,))\n"
     ]
    }
   ],
   "source": [
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02963 0.03083\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.096000    validation acc: 0.172000  loss: 2.516770\n",
      "train acc: 0.314000    validation acc: 0.355000  loss: 2.071633\n",
      "train acc: 0.370000    validation acc: 0.372000  loss: 1.918441\n",
      "train acc: 0.407000    validation acc: 0.408000  loss: 1.783918\n",
      "train acc: 0.443000    validation acc: 0.421000  loss: 1.784086\n",
      "0.03481 0.01882\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.075000    validation acc: 0.209000  loss: 2.363344\n",
      "train acc: 0.292000    validation acc: 0.293000  loss: 2.026530\n",
      "train acc: 0.336000    validation acc: 0.363000  loss: 1.921135\n",
      "train acc: 0.394000    validation acc: 0.367000  loss: 1.744373\n",
      "train acc: 0.422000    validation acc: 0.412000  loss: 1.739872\n",
      "0.0264 0.01649\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.090000    validation acc: 0.164000  loss: 2.348893\n",
      "train acc: 0.293000    validation acc: 0.301000  loss: 1.984817\n",
      "train acc: 0.335000    validation acc: 0.362000  loss: 1.862806\n",
      "train acc: 0.362000    validation acc: 0.383000  loss: 1.797328\n",
      "train acc: 0.364000    validation acc: 0.399000  loss: 1.770934\n",
      "0.00265 0.00714\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.086000    validation acc: 0.140000  loss: 2.311207\n",
      "train acc: 0.181000    validation acc: 0.177000  loss: 2.122893\n",
      "train acc: 0.248000    validation acc: 0.260000  loss: 2.008650\n",
      "train acc: 0.288000    validation acc: 0.268000  loss: 1.909708\n",
      "train acc: 0.270000    validation acc: 0.287000  loss: 1.936501\n",
      "0.02429 0.01335\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.090000    validation acc: 0.099000  loss: 2.332909\n",
      "train acc: 0.258000    validation acc: 0.265000  loss: 2.035659\n",
      "train acc: 0.308000    validation acc: 0.313000  loss: 1.925129\n",
      "train acc: 0.330000    validation acc: 0.354000  loss: 1.862032\n",
      "train acc: 0.361000    validation acc: 0.379000  loss: 1.784228\n",
      "0.02766 0.00358\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.116000    validation acc: 0.113000  loss: 2.304749\n",
      "train acc: 0.162000    validation acc: 0.161000  loss: 2.223369\n",
      "train acc: 0.167000    validation acc: 0.174000  loss: 2.096382\n",
      "train acc: 0.158000    validation acc: 0.201000  loss: 2.065493\n",
      "train acc: 0.167000    validation acc: 0.204000  loss: 2.062490\n",
      "0.00433 0.03793\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.136000    validation acc: 0.186000  loss: 2.703315\n",
      "train acc: 0.341000    validation acc: 0.332000  loss: 2.093639\n",
      "train acc: 0.362000    validation acc: 0.387000  loss: 2.040861\n",
      "train acc: 0.380000    validation acc: 0.418000  loss: 1.946814\n",
      "train acc: 0.419000    validation acc: 0.419000  loss: 1.870662\n",
      "0.00165 0.04711\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.092000    validation acc: 0.134000  loss: 3.973824\n",
      "train acc: 0.281000    validation acc: 0.299000  loss: 2.386720\n",
      "train acc: 0.337000    validation acc: 0.355000  loss: 2.234521\n",
      "train acc: 0.378000    validation acc: 0.394000  loss: 2.127855\n",
      "train acc: 0.371000    validation acc: 0.424000  loss: 2.124866\n",
      "0.01473 0.03888\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.103000    validation acc: 0.147000  loss: 2.783703\n",
      "train acc: 0.328000    validation acc: 0.336000  loss: 2.129281\n",
      "train acc: 0.367000    validation acc: 0.379000  loss: 2.011578\n",
      "train acc: 0.387000    validation acc: 0.418000  loss: 1.947498\n",
      "train acc: 0.372000    validation acc: 0.427000  loss: 1.981136\n",
      "0.00024 0.01325\n",
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.076000    validation acc: 0.181000  loss: 2.332385\n",
      "train acc: 0.266000    validation acc: 0.256000  loss: 2.008253\n",
      "train acc: 0.341000    validation acc: 0.323000  loss: 1.882037\n",
      "train acc: 0.336000    validation acc: 0.346000  loss: 1.839032\n",
      "train acc: 0.378000    validation acc: 0.366000  loss: 1.776487\n",
      "BEST:  0.02963 0.03083 0.443\n"
     ]
    }
   ],
   "source": [
    "layer_list = [\"affine\",\"relu\",\"affine\",\"relu\",\"affine\",\"relu\",\"affine\",\"relu\",\"affine\"]\n",
    "layer_dim = [(3*32*32,100),None,(100,100),None,(100,100),None,(100,100),None,(100,10)]\n",
    "loss_type = \"softmax\"\n",
    "\n",
    "\n",
    "\n",
    "# hyper-param optimizer\n",
    "low_bound = 1e-4\n",
    "high_bound = 5e-2\n",
    "check_values = 10\n",
    "epochs = 1\n",
    "\n",
    "max_train = 0\n",
    "best_lr = 0.0\n",
    "best_ws = 0.0\n",
    "\n",
    "for i in range (check_values):\n",
    "    lr = round(np.random.uniform(low_bound,high_bound),5)\n",
    "    ws = round(np.random.uniform(low_bound,high_bound),5)\n",
    "    print(lr, ws)\n",
    "    my_model = model(layer_list, layer_dim, loss_type, learning_rate=1e-3, update_rule=\"adam\", weight_scale=ws, reg=1e-3)\n",
    "    my_trainer = trainer(my_model, data, num_epochs=epochs, batch_size=1000)\n",
    "    my_trainer.train()\n",
    "    if (my_trainer.train_history[-1] > max_train):\n",
    "        max_train = my_trainer.train_history[-1]\n",
    "        best_lr = lr\n",
    "        best_ws = ws\n",
    "\n",
    "print(\"BEST: \", best_lr, best_ws, max_train)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EPOCH: 0 ---------\n",
      "train acc: 0.103000    validation acc: 0.187000  loss: 2.350033\n",
      "train acc: 0.307000    validation acc: 0.328000  loss: 1.983791\n",
      "train acc: 0.326000    validation acc: 0.371000  loss: 1.817383\n",
      "train acc: 0.416000    validation acc: 0.405000  loss: 1.699998\n",
      "train acc: 0.402000    validation acc: 0.421000  loss: 1.660879\n",
      "--------- EPOCH: 1 ---------\n",
      "train acc: 0.461000    validation acc: 0.448000  loss: 1.540730\n",
      "train acc: 0.427000    validation acc: 0.459000  loss: 1.629682\n",
      "train acc: 0.467000    validation acc: 0.474000  loss: 1.506288\n",
      "train acc: 0.463000    validation acc: 0.452000  loss: 1.536243\n",
      "train acc: 0.485000    validation acc: 0.445000  loss: 1.461543\n",
      "--------- EPOCH: 2 ---------\n",
      "train acc: 0.475000    validation acc: 0.470000  loss: 1.496251\n",
      "train acc: 0.486000    validation acc: 0.486000  loss: 1.486529\n",
      "train acc: 0.505000    validation acc: 0.475000  loss: 1.425074\n",
      "train acc: 0.467000    validation acc: 0.493000  loss: 1.509599\n",
      "train acc: 0.515000    validation acc: 0.488000  loss: 1.443931\n",
      "--------- EPOCH: 3 ---------\n",
      "train acc: 0.537000    validation acc: 0.477000  loss: 1.356664\n",
      "train acc: 0.531000    validation acc: 0.485000  loss: 1.362977\n",
      "train acc: 0.526000    validation acc: 0.507000  loss: 1.335473\n",
      "train acc: 0.518000    validation acc: 0.501000  loss: 1.391105\n",
      "train acc: 0.532000    validation acc: 0.485000  loss: 1.376195\n",
      "--------- EPOCH: 4 ---------\n",
      "train acc: 0.546000    validation acc: 0.485000  loss: 1.327655\n",
      "train acc: 0.525000    validation acc: 0.510000  loss: 1.369512\n",
      "train acc: 0.546000    validation acc: 0.520000  loss: 1.305210\n",
      "train acc: 0.535000    validation acc: 0.500000  loss: 1.306763\n",
      "train acc: 0.555000    validation acc: 0.518000  loss: 1.287498\n",
      "--------- EPOCH: 5 ---------\n",
      "train acc: 0.563000    validation acc: 0.512000  loss: 1.245200\n",
      "train acc: 0.550000    validation acc: 0.513000  loss: 1.273745\n",
      "train acc: 0.530000    validation acc: 0.503000  loss: 1.315644\n",
      "train acc: 0.545000    validation acc: 0.512000  loss: 1.287477\n",
      "train acc: 0.584000    validation acc: 0.524000  loss: 1.250789\n",
      "--------- EPOCH: 6 ---------\n",
      "train acc: 0.568000    validation acc: 0.508000  loss: 1.255293\n",
      "train acc: 0.613000    validation acc: 0.503000  loss: 1.129681\n",
      "train acc: 0.576000    validation acc: 0.507000  loss: 1.183082\n",
      "train acc: 0.581000    validation acc: 0.513000  loss: 1.208951\n",
      "train acc: 0.547000    validation acc: 0.507000  loss: 1.334824\n",
      "--------- EPOCH: 7 ---------\n",
      "train acc: 0.588000    validation acc: 0.515000  loss: 1.151972\n",
      "train acc: 0.601000    validation acc: 0.512000  loss: 1.116736\n",
      "train acc: 0.623000    validation acc: 0.510000  loss: 1.104990\n",
      "train acc: 0.609000    validation acc: 0.512000  loss: 1.169680\n",
      "train acc: 0.583000    validation acc: 0.520000  loss: 1.212853\n",
      "--------- EPOCH: 8 ---------\n",
      "train acc: 0.599000    validation acc: 0.519000  loss: 1.129307\n",
      "train acc: 0.604000    validation acc: 0.517000  loss: 1.129928\n",
      "train acc: 0.582000    validation acc: 0.511000  loss: 1.165176\n",
      "train acc: 0.603000    validation acc: 0.524000  loss: 1.147810\n",
      "train acc: 0.616000    validation acc: 0.503000  loss: 1.102967\n",
      "--------- EPOCH: 9 ---------\n",
      "train acc: 0.642000    validation acc: 0.514000  loss: 1.062984\n",
      "train acc: 0.628000    validation acc: 0.523000  loss: 1.096242\n",
      "train acc: 0.606000    validation acc: 0.514000  loss: 1.104972\n",
      "train acc: 0.643000    validation acc: 0.526000  loss: 1.082122\n",
      "train acc: 0.597000    validation acc: 0.497000  loss: 1.146004\n",
      "--------- EPOCH: 10 ---------\n",
      "train acc: 0.640000    validation acc: 0.496000  loss: 1.030398\n",
      "train acc: 0.641000    validation acc: 0.510000  loss: 1.026038\n",
      "train acc: 0.609000    validation acc: 0.524000  loss: 1.128739\n",
      "train acc: 0.627000    validation acc: 0.504000  loss: 1.078887\n",
      "train acc: 0.628000    validation acc: 0.525000  loss: 1.079031\n",
      "--------- EPOCH: 11 ---------\n",
      "train acc: 0.639000    validation acc: 0.530000  loss: 1.024995\n",
      "train acc: 0.657000    validation acc: 0.537000  loss: 0.986961\n",
      "train acc: 0.645000    validation acc: 0.513000  loss: 0.993642\n",
      "train acc: 0.640000    validation acc: 0.522000  loss: 1.046740\n",
      "train acc: 0.639000    validation acc: 0.517000  loss: 1.026191\n",
      "--------- EPOCH: 12 ---------\n",
      "train acc: 0.645000    validation acc: 0.524000  loss: 0.986096\n",
      "train acc: 0.682000    validation acc: 0.513000  loss: 0.963865\n",
      "train acc: 0.642000    validation acc: 0.524000  loss: 1.007991\n",
      "train acc: 0.664000    validation acc: 0.504000  loss: 1.009093\n",
      "train acc: 0.664000    validation acc: 0.512000  loss: 0.972457\n",
      "--------- EPOCH: 13 ---------\n",
      "train acc: 0.683000    validation acc: 0.505000  loss: 0.921375\n",
      "train acc: 0.674000    validation acc: 0.519000  loss: 0.926823\n",
      "train acc: 0.648000    validation acc: 0.513000  loss: 1.007691\n",
      "train acc: 0.667000    validation acc: 0.492000  loss: 0.968055\n",
      "train acc: 0.676000    validation acc: 0.515000  loss: 0.990633\n",
      "--------- EPOCH: 14 ---------\n",
      "train acc: 0.702000    validation acc: 0.506000  loss: 0.868735\n",
      "train acc: 0.686000    validation acc: 0.516000  loss: 0.919416\n",
      "train acc: 0.697000    validation acc: 0.531000  loss: 0.880335\n",
      "train acc: 0.671000    validation acc: 0.526000  loss: 0.960082\n",
      "train acc: 0.647000    validation acc: 0.524000  loss: 1.026779\n"
     ]
    }
   ],
   "source": [
    "my_model = model(layer_list, layer_dim, loss_type, learning_rate=1e-3, update_rule=\"adam\", weight_scale=best_ws, reg=1e-4)\n",
    "my_trainer = trainer(my_model, data, num_epochs=15, batch_size=1000)\n",
    "my_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  1.09662762797e-07\n",
      "v error:  4.20831403811e-09\n",
      "m error:  4.21496319311e-09\n"
     ]
    }
   ],
   "source": [
    "from updater import * \n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "W = {}\n",
    "DW = {}\n",
    "M = {}\n",
    "V = {}\n",
    "W[\"w\"] = w\n",
    "DW[\"w\"] = dw\n",
    "M[\"w\"] = m\n",
    "V[\"w\"] = v\n",
    "\n",
    "\n",
    "my_updater = updater(1e-2,\"adam\",params=W)\n",
    "my_updater.first_moment = M\n",
    "my_updater.sec_moment = V\n",
    "my_updater.itr = 5\n",
    "\n",
    "\n",
    "x = my_updater.adam(W, DW)\n",
    "next_w = x[\"w\"]\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print ('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print ('v error: ', rel_error(expected_v, my_updater.sec_moment[\"w\"]))\n",
    "print ('m error: ', rel_error(expected_m, my_updater.first_moment[\"w\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_w error:  9.78754489423e-08\n",
      "cache error:  2.64779558072e-09\n"
     ]
    }
   ],
   "source": [
    "from updater import *\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "W = {}\n",
    "DW = {}\n",
    "sq = {}\n",
    "W[\"w\"] = w\n",
    "DW[\"w\"] = dw\n",
    "sq[\"w\"] = cache\n",
    "\n",
    "\n",
    "\n",
    "my_updater = updater(1e-2,\"rmsprop\",params=W)\n",
    "my_updater.grads_sqrd = sq\n",
    "x = my_updater.rmsprop(W, DW)\n",
    "next_w = x[\"w\"]\n",
    "\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache,  my_updater.grads_sqrd['w']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "3.85824705518e-11\n",
      "Running numeric gradient check with reg =  0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-74ba00427dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mgrad_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "\n",
    "layer_list = [\"affine\",\"relu\",\"affine\"]\n",
    "layer_dim = [(D,H),None,(H,C)]\n",
    "loss_type = \"softmax\"\n",
    "\n",
    "my_model = model(layer_list, layer_dim, loss_type, weight_scale=std, reg=0)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(my_model.layers[0].params['W'].std() - std)\n",
    "b1 = my_model.layers[0].params['b']\n",
    "W2_std = abs(my_model.layers[2].params['W'].std() - std)\n",
    "b2 = my_model.layers[2].params['b']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "my_model.layers[0].params['W'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "my_model.layers[0].params['b'] = np.linspace(-0.1, 0.9, num=H)\n",
    "my_model.layers[2].params['W'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "my_model.layers[2].params['b'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores, w = my_model.forward_pass(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "grads, loss = my_model.calculate_loss(scores, y, w)\n",
    "correct_loss = 3.4702243556\n",
    "\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "my_model.reg = 1.0\n",
    "for layer in my_model.layers:\n",
    "    layer.reg = 1.0\n",
    "\n",
    "scores, w = my_model.forward_pass(X)\n",
    "grads, loss = my_model.calculate_loss(scores, y, w)\n",
    "correct_loss = 26.5948426952\n",
    "print(abs(loss - correct_loss))\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:update rule  not found, updater not properly init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [ 26.55068073 -13.66394309   0.92234417]\n",
      "  stds:  [ 34.83511196  33.69398718  24.00501513]\n",
      "After batch normalization (gamma=1, beta=0)\n",
      "  mean:  [ -5.88626370e-16  -1.38777878e-17  -3.33066907e-18]\n",
      "  std:  [ 1.          1.          0.99999999]\n",
      "After batch normalization (nontrivial gamma, beta)\n",
      "  means:  [ 11.  12.  13.]\n",
      "  stds:  [ 1.          1.99999999  2.99999997]\n"
     ]
    }
   ],
   "source": [
    "from layers import * \n",
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print ('Before batch normalization:')\n",
    "print ('  means: ', a.mean(axis=0))\n",
    "print ('  stds: ', a.std(axis=0))\n",
    "bnl = batch_norm_layer((D3,D3), 1e-3, \"\", \"\")\n",
    "bnl.params[\"gamma\"] = np.ones(D3)\n",
    "bnl.params[\"beta\"] = np.zeros(D3)\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print ('After batch normalization (gamma=1, beta=0)')\n",
    "a_norm = bnl.step_forward(a)\n",
    "print ('  mean: ', a_norm.mean(axis=0))\n",
    "print ('  std: ', a_norm.std(axis=0))\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "bnl.params[\"gamma\"] = gamma\n",
    "bnl.params[\"beta\"] = beta\n",
    "a_norm = bnl.step_forward(a)\n",
    "print ('After batch normalization (nontrivial gamma, beta)')\n",
    "print ('  means: ', a_norm.mean(axis=0))\n",
    "print ('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
